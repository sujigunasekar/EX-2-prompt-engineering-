# EX-5-prompt-engineering-Comparative Analysis of different types of Prompting patterns and explain with Various Test scenerios

Test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios. 
     Analyze the quality, accuracy, and depth of the generated responses.


# Objective:
The objective of this experiment is to explore and compare how different prompting patterns influence the responses generated by large language models (LLMs). The focus is on evaluating response quality in terms of accuracy, relevance, depth, coherence, and creativity. The experiment demonstrates how effective prompting can significantly enhance the performance of LLMs in various scenarios.

# Prompting Techniques Covered
In this experiment, six major types of prompting patterns are examined:

Zero-shot prompting involves asking a question or requesting a task without providing any example or context.

One-shot prompting offers a single example to guide the model in understanding the desired output format.

Few-shot prompting provides multiple examples, helping the model better identify the intended structure or logic.

Chain-of-thought prompting encourages the model to break down reasoning steps before arriving at an answer, improving logical consistency.

Role prompting involves assigning the model a specific persona or role to influence tone, behavior, or domain knowledge.

Instruction prompting gives a clear, directive instruction outlining exactly what needs to be done.
# Test Scenarios
## Scenario 1: Math Problem Solving
#### Prompt Type: Zero-shot

Prompt: "What is 37 × 89?"

#### ChatGPT Response: 
“The answer is 3293.”

#### Gemini Response: 
“37 multiplied by 89 is 3293.”

#### Prompt Type: Chain-of-Thought

Prompt: "Solve step by step: What is 37 × 89?"

#### ChatGPT Response:
“Let’s break it down:
30 × 89 = 2670
7 × 89 = 623
Now, 2670 + 623 = 3293.”

#### Gemini Response:
“First, split 37 into 30 and 7.
30 × 89 = 2670
7 × 89 = 623
Final answer: 2670 + 623 = 3293.”

## Scenario 2: Email Writing
#### Prompt Type: Broad

Prompt: "Write an email to my professor."

#### ChatGPT Response:
“Hi Professor,
I hope you're doing well. I wanted to ask about our assignment…”

#### Gemini Response:
“Dear Professor,
I have a few questions regarding our recent lecture…”

#### Prompt Type: Role + Instruction

Prompt: "You are a polite university student. Write a formal email to your professor requesting an extension on your assignment due to illness."

#### ChatGPT Response:
“Dear Professor [Last Name],
I hope this email finds you well. I am writing to request a short extension for the assignment due on [date] as I have been unwell…”

#### Gemini Response:
“Dear Professor,
I’m currently recovering from an illness and would appreciate a deadline extension for the upcoming assignment…”

## Scenario 3: Summarization
#### Prompt Type: Zero-shot

#### Prompt: "Summarize this article."

#### ChatGPT Response:
“This article discusses recent developments in climate policy, highlighting global cooperation efforts.”

#### Gemini Response:
“The article is about climate change and mentions actions taken by different countries.”

#### Prompt Type: Instruction

Prompt: "Summarize this article in 3 sentences, focusing on causes and consequences."

#### ChatGPT Response:
“The article explains that industrial emissions are a major cause of global warming. It discusses international responses and policies to reduce emissions. The consequences include rising sea levels and extreme weather.”

#### Gemini Response:
“The article describes emissions as the main cause of climate change. It also shows how this leads to severe weather and rising oceans. Countries are working together to address this issue.”

## Scenario 4: Coding Task
#### Prompt Type: Unstructured

Prompt: "Write Python code for sorting."

#### ChatGPT Response:

python
```
my_list = [5, 2, 1, 4]
my_list.sort()
print(my_list)
```
#### Gemini Response:

python
```
numbers = [3, 1, 4, 2]
print(sorted(numbers))
```
#### Prompt Type: Few-shot
Prompt: "Write a Python function using Bubble Sort to sort the list [4, 1, 5, 2]. The expected output is [1, 2, 4, 5]."

#### ChatGPT Response:

python
```
def bubble_sort(lst):
    n = len(lst)
    for i in range(n):
        for j in range(0, n-i-1):
            if lst[j] > lst[j+1]:
                lst[j], lst[j+1] = lst[j+1], lst[j]
    return lst

print(bubble_sort([4, 1, 5, 2]))
```
#### Gemini Response:

python
```
def bubble_sort(nums):
    for i in range(len(nums)):
        for j in range(0, len(nums)-i-1):
            if nums[j] > nums[j+1]:
                nums[j], nums[j+1] = nums[j+1], nums[j]
    return nums

bubble_sort([4, 1, 5, 2])
```
## Scenario 5: Fact-Based Q&A
#### Prompt Type: Basic

Prompt: "Tell me about Mars."

#### ChatGPT Response:
“Mars is the fourth planet from the Sun and is often called the Red Planet. It has a thin atmosphere composed mostly of carbon dioxide and surface features similar to both the Moon and Earth.”

#### Gemini Response:
“Mars is known as the Red Planet. It’s the fourth planet from the Sun and has dry, rocky terrain with polar ice caps.”

#### Prompt Type: Instruction

Prompt: "List three scientific facts about Mars, including its atmosphere, temperature, and the possibility of life."

#### ChatGPT Response:
“1. Mars has a thin atmosphere made primarily of carbon dioxide.
2. Its average surface temperature is around -63°C.
3. There is no confirmed life, but evidence suggests Mars once had liquid water.”

#### Gemini Response:
“1. Mars has a CO2-dominated thin atmosphere.
2. Surface temperatures average about -60°C.
3. Signs of past water suggest it might have supported microbial life.”

# RESULT
Broad or unstructured prompts tend to produce faster responses but are often less reliable, with occasional factual inaccuracies or shallow content.
Refined prompting patterns such as chain-of-thought, few-shot, role-based, and instruction-based prompting significantly improve the quality of output.
These approaches make the responses more accurate, relevant, and coherent.
