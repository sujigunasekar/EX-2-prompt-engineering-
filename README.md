# EX-5-prompt-engineering-Comparative Analysis of different types of Prompting patterns and explain with Various Test scenerios

Test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios. 
     Analyze the quality, accuracy, and depth of the generated responses.


# Objective:
The objective of this experiment is to explore and compare how different prompting patterns influence the responses generated by large language models (LLMs). The focus is on evaluating response quality in terms of accuracy, relevance, depth, coherence, and creativity. The experiment demonstrates how effective prompting can significantly enhance the performance of LLMs in various scenarios.

# Prompting Techniques Covered
In this experiment, six major types of prompting patterns are examined:

Zero-shot prompting involves asking a question or requesting a task without providing any example or context.

One-shot prompting offers a single example to guide the model in understanding the desired output format.

Few-shot prompting provides multiple examples, helping the model better identify the intended structure or logic.

Chain-of-thought prompting encourages the model to break down reasoning steps before arriving at an answer, improving logical consistency.

Role prompting involves assigning the model a specific persona or role to influence tone, behavior, or domain knowledge.

Instruction prompting gives a clear, directive instruction outlining exactly what needs to be done.
# Test Scenarios
### 1. Math Problem Solving
In this scenario, a basic multiplication problem (37 multiplied by 89) is used to test the effect of zero-shot versus chain-of-thought prompting.

With a zero-shot prompt like “What is 37 × 89?”, the model tends to output a quick answer but may make calculation errors due to lack of intermediate reasoning.

When using chain-of-thought prompting, such as “Solve step by step: What is 37 × 89?”, the model explains its reasoning process and typically arrives at the correct result.

### 2. Email Writing
This scenario examines how an email draft changes based on prompt style.

A broad, unstructured prompt like “Write an email to my professor” often results in an overly generic or casual message.

In contrast, a role + instruction prompt such as “You are a polite university student. Write a formal email to your professor requesting an extension on your assignment due to illness,” produces a well-structured, respectful, and context-appropriate email.

### 3. Summarization
Here, the goal is to see how models perform when asked to summarize a news article.

A zero-shot prompt like “Summarize this article” typically leads to vague or superficial summaries, lacking focus on key points.

An instructional prompt, such as “Summarize this article in 3 sentences, focusing on causes and consequences,” leads to more relevant, targeted summaries that capture the most important information.

### 4. Coding Task
This scenario explores how LLMs perform when asked to write code for sorting a list.

An unstructured prompt like “Write Python code for sorting” often results in a quick response using Python’s built-in sort() method.

A few-shot prompt provides a specific requirement and an example, such as: “Write a Python function using the Bubble Sort algorithm to sort the list [4, 1, 5, 2]. The expected output is [1, 2, 4, 5].” This guides the model to use the correct algorithm and produce code that matches the requirement.

### 5. Fact-Based Q&A
This scenario tests the model’s ability to provide factual information about Mars.

A basic prompt like “Tell me about Mars” usually returns a general overview with mixed or loosely related facts.

An instructional prompt, for example “List three scientific facts about Mars, including its atmosphere, temperature, and the possibility of life,” ensures that the response is well-organized, accurate, and directly answers the request.

# Evaluation Criteria
Responses in each scenario are evaluated based on the following:

Accuracy: How factually correct is the content?

Relevance: Does the response directly address the request?

Depth: How detailed and insightful is the output?

Coherence: Is the language fluent and logically structured?

Creativity: Does the model present the information in a thoughtful or engaging way?

# RESULT
Broad or unstructured prompts tend to produce faster responses but are often less reliable, with occasional factual inaccuracies or shallow content.
Refined prompting patterns such as chain-of-thought, few-shot, role-based, and instruction-based prompting significantly improve the quality of output.
These approaches make the responses more accurate, relevant, and coherent.
